library(here)
library(tidyverse)
library(psych)
dat1 <- read_delim(here("Data", "SNF_2019_combined.csv"), delim = ";", escape_double = FALSE, trim_ws = TRUE)
dat2 <- read_delim(here("Data", "SNF_2020_combined.csv"), delim = ";", escape_double = FALSE, trim_ws = TRUE)
glimpse(dat2)
table(dat1$Form)
distinct(as.character(dat1$Participant))
dat1 <- dat1 %>% mutate(ID = as.character(dat1$Participant))
# in study 2, two versions for each ESM variable exist; I don't know why but let's merge it for now
dat2 <- dat2 %>% mutate(ID = as.character(dat2$Participant),
Con_Exp = A_Con_Exp,
Con_Exp = ifelse(is.na(Con_Exp), B_Con_Exp, A_Con_Exp))
length(unique(dat1$ID)) # 234
length(unique(dat2$ID)) # 587  column partly contains feedback to the study
#only include data points in which participants have reported one conflict
conflict_dat1 <- dat1 %>% filter(Con_Exp %in% c(1,2,3))
conflict_dat2 <- dat2 %>% filter(Con_Exp %in% c(1,2,3))
conflict_dat2 <- dat2 %>% filter(Con_Exp %in% c(1,2,3))
View(conflict_dat2)
distinct(conflict_dat2, Participant)
length(distinct(conflict_dat2, Participant))
n <- distinct(conflict_dat2, Participant)
knitr::opts_chunk$set(echo = TRUE)
pipeline_ps <- ParamSet$new(list(ParamDbl$new("lambda", lower = 0, upper = 1),
ParamDbl$new("alpha", lower = 0, upper = 1)))
library(mlr3verse)
library(mlr3verse)
library(glmnet)
library(mlr3verse)
library(glmnet)
library(DALEX)
library(here)
library(purrr)
# not sure whether I really need those packages
library(rpart.plot)
library(paradox)
library(future)
m_data <- read_csv(here("Data", "study01_finaldata.csv"))
data <- m_data %>% select(starts_with("m_")|# mindfulness items
starts_with("C_NEO")) # c items
# computing facet scores
data <- data %>% mutate(co_achiev = rowMeans(select(., starts_with("C_NEO_achiev"))),
co_compt= rowMeans(select(., starts_with("C_NEO_compt"))),
co_delib = rowMeans(select(., starts_with("C_NEO_delib"))),
co_disc = rowMeans(select(., starts_with("C_NEO_disc"))),
co_dut = rowMeans(select(., starts_with("C_NEO_dut"))),
co_ord = rowMeans(select(., starts_with("C_NEO_ord"))))
# recode items
recode_items <- data %>% select(ends_with("_r")) %>% names()
data[recode_items] <- 8 - data [recode_items]
# remove participants with missing data
data <- na.omit(data)
# remove all C items
data <- data %>% select(-starts_with("C_NEO"))
#standardize variables
data <- data %>% mutate(across(everything(), ~ as.vector(scale(.))))
#standardize variables
data <- data %>% mutate(across(everything(), ~ as.vector(scale(.))))
# Creating a task object
task_achiev <- as_task_regr(data, id ="achiev_Lasso", target = "co_achiev") # what stands the ID for again?
# remove the c_facets from the features
task_achiev$set_col_roles(c("co_compt", "co_delib", "co_disc", "co_dut", "co_ord"), remove_from = "feature")
### Try out a different outcome
task_compt <- as_task_regr(data, id="compt_Lasso", target ="co_compt")
task_compt$set_col_roles(c("co_achiev", "co_delib", "co_disc", "co_dut", "co_ord"), remove_from = "feature")
library(mlr3verse)
library(glmnet)
library(DALEX)
library(tidyverse)
library(here)
library(purrr)
# not sure whether I really need those packages
library(rpart.plot)
library(paradox)
library(future)
# not sure whether I really need those packages
library(rpart.plot)
library(paradox)
library(future)
## Load and prepapre data
Data taken from Altgassen et al. (2023). For the pilot analysis, the 173 mindfulness items will be used as predictor items. As outcomes, facets (7) of conscientiousness were chosen as this dimension demonstrated the strongest relationship with a general mindfulness factor. Only complete cases are used (N= 434). All (predictor) variables have to be standardized.
```{r}
m_data <- read_csv(here("Data", "study01_finaldata.csv"))
m_data <- read_csv(here("Data", "study01_finaldata.csv"))
data <- m_data %>% select(starts_with("m_")|# mindfulness items
starts_with("C_NEO")) # c items
# computing facet scores
data <- data %>% mutate(co_achiev = rowMeans(select(., starts_with("C_NEO_achiev"))),
co_compt= rowMeans(select(., starts_with("C_NEO_compt"))),
co_delib = rowMeans(select(., starts_with("C_NEO_delib"))),
co_disc = rowMeans(select(., starts_with("C_NEO_disc"))),
co_dut = rowMeans(select(., starts_with("C_NEO_dut"))),
co_ord = rowMeans(select(., starts_with("C_NEO_ord"))))
# recode items
recode_items <- data %>% select(ends_with("_r")) %>% names()
data[recode_items] <- 8 - data [recode_items]
# remove participants with missing data
data <- na.omit(data)
# remove all C items
data <- data %>% select(-starts_with("C_NEO"))
#standardize variables
data <- data %>% mutate(across(everything(), ~ as.vector(scale(.))))
#standardize variables
data <- data %>% mutate(across(everything(), ~ as.vector(scale(.))))
# performance evaluation with k-fold cross-validation
Step 1 from the tutorial. Try out with the co_achiev outcome
## Creating the task object
```{r}
# Creating a task object
task_achiev <- as_task_regr(data, id ="achiev_Lasso", target = "co_achiev") # what stands the ID for again?
# remove the c_facets from the features
task_achiev$set_col_roles(c("co_compt", "co_delib", "co_disc", "co_dut", "co_ord"), remove_from = "feature")
### Try out a different outcome
task_compt <- as_task_regr(data, id="compt_Lasso", target ="co_compt")
task_compt$set_col_roles(c("co_achiev", "co_delib", "co_disc", "co_dut", "co_ord"), remove_from = "feature")
# specify resampling strategy
set.seed(1)
pipeline_ps <- ParamSet$new(list(ParamDbl$new("lambda", lower = 0, upper = 1),
ParamDbl$new("alpha", lower = 0, upper = 1)))
res_inner <- rsmp("cv", folds = 5)
pipeline_ps <- ParamSet$new(list(ParamDbl$new("lambda", lower = 0.001, upper = 1),
ParamDbl$new("alpha", lower = 0, upper = 1)))
res_inner <- rsmp("cv", folds = 5)
tuner <- tnr("grid_search", resolution =1000) # define tuning method
mes_inner <- msr("regr.mse")
terminator <- trm("evals") # define stopping criteria; consider increasing when I have enough computational power
#tuner <- tnr("random_search") # define tuning method
tuner <- tnr("grid_search", resolution =1000) # define tuning method
learner_el <- lrn("regr.glmnet")
# setting up the configuration for the autotuner
pipeline_at <- AutoTuner$new(
learner = learner_el,
resampling = res_inner,
measure = mes_inner,
search_space = pipeline_ps,
terminator = terminator,
tuner = tuner,
store_models = FALSE)
n_folds <- 10
# execute
plan("multisession", workers = 2) # sets up parallel processing with 2 workers
set.seed(2)
res_outer <- rsmp("cv", folds = n_folds)
invisible({ #used to suppress console output
capture.output({
nested_res <- resample(task = task_achiev, learner = pipeline_at, resampling = res_outer, store_models = TRUE)
})
})
pipeline_ps <- ParamSet$new(list(ParamDbl$new("lambda", lower = 0.001, upper = 1),
ParamDbl$new("alpha", lower = 0, upper = 1)))
res_inner <- rsmp("cv", folds = 5)
tuner <- tnr("grid_search", resolution =5000) # define tuning method
mes_inner <- msr("regr.mse")
learner_el <- lrn("regr.glmnet")
# setting up the configuration for the autotuner
pipeline_at <- AutoTuner$new(
learner = learner_el,
resampling = res_inner,
measure = mes_inner,
search_space = pipeline_ps,
terminator = terminator,
tuner = tuner,
store_models = FALSE)
n_folds <- 10
# execute
plan("multisession", workers = 2) # sets up parallel processing with 2 workers
set.seed(2)
res_outer <- rsmp("cv", folds = n_folds)
invisible({ #used to suppress console output
capture.output({
nested_res <- resample(task = task_achiev, learner = pipeline_at, resampling = res_outer, store_models = TRUE)
})
})
#performance; consider changing the performance criterion later
mes_outer <- msr("regr.mse")
nested_res$aggregate(mes_outer)
nested_res$resampling
nested_res$aggregate(msr("regr.rsq")) # R2
#inspecting coefficients
trained_models <- nested_res$learners
coef_ls <- list()
vec_folds <- seq(1, n_folds, 1)
for(n_f in vec_folds){
model <- trained_models[[n_f]]$model[[1]] # check the results of the first fold
coef_model <- model$model
coef_output <- as.matrix(coef_model$beta) %>% data.frame() # coef grind search
# coef_output <- as.matrix(coef(coef_model)) %>% data.frame() # coef random search
names(coef_output) <- paste0("coef")
coef_output$vars <- row.names(coef_output)
coef_output <- coef_output[-1,] # remove intercept
coef_ls[[paste0("all_", n_f)]] <- coef_output
coef_ls[[paste0("no_zero_", n_f)]] <- coef_output %>% filter(pull(., 1)!=0) # only contain rows where the coefficient is not zero
}
# compare the outcomes across folds
## performance measures
predictions_list <- nested_res$predictions()
cor_values <- lapply(predictions_list, function(pred) {
cor(pred$truth, pred$response)
})
round(unlist(cor_values), 3)
round(nested_res$score(msr("regr.rsq"))$regr.rsq,3)
round(nested_res$score(msr("regr.mse"))$regr.mse,3)
## hyperparameter
all_lambdas <-c()
for(n_k in vec_folds) {
all_lambdas[[n_k]] <- nested_res$learners[[n_k]]$tuning_result$lambda
}
round(unlist(all_lambdas),4)
all_alphas <-c()
for(n_k in vec_folds) {
all_alphas[[n_k]] <- nested_res$learners[[n_k]]$tuning_result$alpha
}
round(unlist(all_alphas), 4)
